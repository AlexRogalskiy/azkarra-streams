[{"body":"This section describes the main concepts Azkarra uses to execute a Kafka Streams instance.\nTopology The concept of Topology is not specific to Azkarra but is fundamental in the implementation of a Kafka Streams application. A Topology is an object that is part of the public Kafka Streams API and allows you to define all the operations (i.e stateless or stateful) to be performed on one or more Kafka topics.\nIn Azkarra Sterams, a Topology object must be defined and provided through the implementation of the TopologyProvider interface.\nHere is a simple example :\npublic class WordCountTopology implements TopologyProvider { @Override public Topology get() { StreamsBuilder builder = new StreamsBuilder(); KStream\u0026lt;String, String\u0026gt; textLines = builder.stream(\u0026#34;streams-plaintext-input\u0026#34;); textLines .flatMapValues(value -\u0026gt; Arrays.asList(value.toLowerCase().split(\u0026#34;\\\\W+\u0026#34;)) ) .groupBy((key, value) -\u0026gt; value) .count(Materialized.as(\u0026#34;WordCount\u0026#34;)) .toStream() .to( \u0026#34;streams-wordcount-output\u0026#34;, Produced.with(Serdes.String(), Serdes.Long()) ); return builder.build(); } @Override public String version() { return \u0026#34;1.0\u0026#34;; } } One of the particularities in Azkarra, is that the TopologyProvider interface enforces you to provide a non-null topology version. Usually, you will return the version of your project (e.g : 1.0, 1.2-SNAPSHOT, etc). Azkarra uses this version to generate a meaningful config application.id for your streams instance if no one is provided at runtime.\nComponent In Azkarra, any object that forms your streams application (e.g: TopologyProviders) and is registered to and managed by an AzkarraContext instance is called a component.\nMore generally, a component is an object that is instantiated, assembled, and configured by a AzkarraContext instance.\nThe concepts of component enable the implementation of the dependency injection pattern in Azkarra.\nStreamsExecutionEnvironment Azkarra uses the concept of StreamExecutionEnvironment to safely execute your Topologies. Indeed, you are not anymore responsible to create and start a new KafkaStreams instance by yourself. This is done by the StreamExecutionEnvironment.\nA StreamExecutionEnvironment is responsible for creating, configuring and starting a new KafkaStreams instance for each Topology you provide.\nHere is a simple example to execute the topology we illustrated in the previous section.\n// (1) define your Kafka Streams configuration. Map\u0026lt;String, Object\u0026gt; props = new HashMap\u0026lt;\u0026gt;(); props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \u0026#34;localhost:9092\u0026#34;); props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass()); props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass()); // (2) create a new execution environment. StreamsExecutionEnvironment env = DefaultStreamsExecutionEnvironment.create(Conf.with(\u0026#34;streams\u0026#34;, props)); // (3) register the topology to run. env.addTopology(WordCountTopology::new, Executed.as(\u0026#34;wordcount\u0026#34;)); env.start(); // (4) start the KafkaStreams instance. A StreamExecutionEnvironment instance can manage multiple KafkaStreams instances and you can also create multiple environments.\nAn environment allows you to define common configuration properties, listeners and behaviors to apply on a group of Kafka Streams instances\nMoreover, an Azkarra application is not limited to a single environment. For example, if during the development phase you need to quickly test a Kafka Streams application on different Kafka clusters, you can create one StreamsExecutionEnvironment for each target Kafka Cluster.\nIn the code example above, you can notice that we name the topology to be executed. In the same way, we can also name an environment.\nDefaultStreamsExecutionEnvironment.create(Conf.with(\u0026quot;streams\u0026quot;, props), \u0026quot;dev\u0026quot;); Azkarra uses this information to generate the application.id property to set for the Kafka Streams application. In our case, the generated id will be dev-wordcount-1.0.\nAzkarraContext A AzkarraContext object is responsible for configuring and running one ore more StreamsExecutionEnvironment. In addition, the AzkarraContext allows you to quickly configure a Java shutdown hook to ensure that our JVM shutdowns are handled gracefully.\nA context can be created as follows :\nDefaultAzkarraContext.create(config) .addExecutionEnvironment(env) .setRegisterShutdownHook(true) .start(); By default, an AzkarraContext will always create a default StreamsExecutionEnvironment named __default. This allows you to register the TopologyProvider directly at the context level.\ncontext.addTopology(WordCountTopologyProvider.class, Executed.as(\u0026#34;wordcount\u0026#34;)) Registering a TopoloyProvider at the context level has some advantages. Indeed, this allows you to use the Java annotations provided by the Azkarra framework (e.g @TopologyInfo, @DefaultStreamsConfig). See Configuration for more information.\nAzkarraApplication The AzkarraApplication is the top-level concept that is used to bootstrap an Azkarra Streams application.\nIts main responsibility is to initialize an AzkarraContext using a user-defined configuration which describes the StreamsExecutionEnvironments and the streams applications to be executed.\nThe AzkarraApplication is also responsible for deploying an embedded HTTP server (if enable) which exposes the REST endpoints that can be used to manage registered topologies and to run streams instances.\n","excerpt":"This section describes the main concepts Azkarra uses to execute a Kafka Streams instance.\nTopology …","ref":"https://streamthoughts.github.io/azkarra-streams/docs/developer-guide/concepts/","title":"Concepts"},{"body":"What is it? Azkarra Streams is a lightweight Java framework which makes easy to develop and operate Kafka Streams applications (Azkarra is Basque words meaning \u0026ldquo;Fast\u0026rdquo;).\nAbout Kafka Streams Kafka Streams is a client library for building applications and microservices, where the input and output data are stored in Kafka clusters. It combines the simplicity of writing and deploying standard Java and Scala applications on the client side with the benefits of Kafka's server-side cluster technology (source: Apache documentation).  Key Features Azkarra Streams provides a set of features to quickly debug and build production-ready Kafka Streams applications. This includes, among other things:\n Lifecycle management of Kafka Streams instances (no more KafkaStreams#start()). Easy externalization of configurations (using Typesafe Config). Embedded HTTP server for querying state stores. HTTP endpoints to monitor streams application metrics (e.g : JSON, Prometheus). Embedded Web user interface for the visualization of DAGs of topologies Encryption and Authentication with SSL or Basic Auth. Etc.  Why do I want it? Azkarra helps you build Kafka Streams applications using best pratices developped by the industry.\n  What is it good for?: Azkarra lets you focus on writing Kafka Streams topologies code, not boilerplate code necessary for executing them.\n  What is it not good for?: Azkarra is not attented to be used for operating a fleet of Kafka Streams applications.\n  What is it not yet good for?: Azkarra cannot be used for managing a distributed Kafka Streams application.\n  Where should I go next? Give your users next steps from the Overview. For example:\n Getting Started: Get started with Azkarra Streams Examples: Check out some example code!  ","excerpt":"What is it? Azkarra Streams is a lightweight Java framework which makes easy to develop and operate …","ref":"https://streamthoughts.github.io/azkarra-streams/docs/overview/","title":"Overview"},{"body":"In this tutorial we will explore the main Azkarra APIs step by step. For that purpose we are going to develop a Kafka Streams application using the famous WordCountTopology example.\nThe prerequisites for this tutorial are :\n IDE or Text editor. Java 11 Maven 3+ Docker (for running a Kafka Cluster 2.x).  Setting up a Maven Project We are going to use a Azkarra Streams Maven Archetype for creating a simple project structure. First, run this following command :\n$\u0026gt; mvn archetype:generate -DarchetypeGroupId=io.streamthoughts \\ -DarchetypeArtifactId=azkarra-quickstart-java \\ -DarchetypeVersion=0.5.0 \\ -DgroupId=azkarra.streams \\ -DartifactId=azkarra-getting-started \\ -Dversion=1.0-SNAPSHOT \\ -Dpackage=azkarra \\ -DinteractiveMode=false Maven will create a new project with the following structure :\n$\u0026gt; tree azkarra-getting-started azkarra-getting-started ├── docker-compose.yml ├── pom.xml ├── quickstart-create-wordcount-topics.sh └── src └── main ├── java │ └── azkarra │ ├── SimpleStreamsApp.java │ └── Version.java └── resources ├── application.conf ├── log4j2.xml └── version.properties The pom.xml already contains the Azkarra Streams and Kafka Streams dependencies :\n\u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.kafka\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;kafka-streams\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.4.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.streamthoughts\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;azkarra-streams\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.5.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; By default, Maven resources files are add to src/main/resources The archetype contains a default configuration file application.conf that we can ignore for the moment. We will come back to this later.\nThe project also contains a very straightforward docker-compose.yml that we are going to use for setting up a single node Kafka cluster\nWriting Your First App Using your favorite IDE or editor, open the Maven project. For more fun, we will remove the bundle example and start our first app from scratch.\n$\u0026gt; cd azkarra-getting-started $\u0026gt; rm -rf src/main/java/azkarra/*.java First, let's create a new file src/main/java/azkarra/SimpleStreamsApp.java with a basic java main(String[] args) method.\npublic class SimpleStreamsApp { public static void main(final String[] args) { } } In Azkarra, we have to implement the TopologyProvider interface in order to return a Topology instance through a method get(). For our example, we will start by creating a new class named WordCountTopologyProvider which implements this interface. To keep it simple, you can declare a nested class into SimpleStreamsApp.\npublic static class WordCountTopologyProvider implements TopologyProvider { @Override public String version() { return \u0026#34;1.0-SNAPSHOT\u0026#34;; } @Override public Topology get() { final StreamsBuilder builder = new StreamsBuilder(); final KStream\u0026lt;String, String\u0026gt; source = builder.stream(\u0026#34;streams-plaintext-input\u0026#34;); final KTable\u0026lt;String, Long\u0026gt; counts = source .flatMapValues(value -\u0026gt; Arrays.asList(value.toLowerCase().split(\u0026#34;\\\\W+\u0026#34;))) .groupBy((key, value) -\u0026gt; value) .count(Materialized.as(\u0026#34;count\u0026#34;)); counts.toStream().to( \u0026#34;streams-wordcount-output\u0026#34;, Produced.with(Serdes.String(), Serdes.Long()) ); return builder.build(); } } (Note that for clarity, all import statements are omitted in code snippets. At the end of this section, we will show the complete code.)\nAs you can see, the TopologyProvider interface enforces you to provide a topology version. The version is required and you can't return null. Usually, you will return the version of your project (e.g : 1.0, 1.2-SNAPSHOT, etc). Azkarra will use this version to generate a meaningful config application.id for your streams instance if no one is provided at runtime.\nWith Azkarra, you doesn't have to create a new KafkaStreams instance to run the Topology. This is done internally by the Azkarra API.\nInstead of that, you are going to create a new StreamsExecutionEnvironment instance in the main method. The StreamsExecutionEnvironment is used to handle the lifecycle of one or multiple KafkaStreams instances. Furthermore, this can be used for setting common configuration and/or registering multiple listeners to be applied on all running Kafka Streams instances.\nStreamsExecutionEnvironment env = DefaultStreamsExecutionEnvironment.create(); The code above, will create a new environment with an empty configuration.\nMoreover, an Azkarra application is not limited to a single environment. For example, if you need to run a Kafka Streams application multiple time, you can create one environment for each target Kafka Cluster. Thus, each environment will have its own configuration which can be passed through the method DefaultStreamsExecutionEnvironment#create.\nNext, let's define the configuration that we are going to use for deploying the application.\nAdd the following lines of code in the main() method :\n// Then, define the Kafka Streams configuration. Map\u0026lt;String, Object\u0026gt; props = new HashMap\u0026lt;\u0026gt;(); props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \u0026#34;localhost:9092\u0026#34;); props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass()); props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass()); Then, we can register the WordCountTopologyProvider previously define to our environment.\nConf config = Conf.with(\u0026#34;streams\u0026#34;, props); env.addTopology(WordCountTopologyProvider::new, Executed.as(\u0026#34;wordcount\u0026#34;).withConfig(config)); The streams configuration is passed through an Executed object. This object can also be used for naming or describing your topology.\nNote that with Azkarra, we don't manipulateMap or Properties objects to configure a Kafka Streams application but we use a Conf object.\nFinally, we are going to start the environment. Note, that the start() method is always non-blocking.\nenv.start(); The complete code so far is this:\npackage azkarra; import io.streamthoughts.azkarra.api.Executed; import io.streamthoughts.azkarra.api.StreamsExecutionEnvironment; import io.streamthoughts.azkarra.api.config.Conf; import io.streamthoughts.azkarra.api.streams.TopologyProvider; import io.streamthoughts.azkarra.runtime.env.DefaultStreamsExecutionEnvironment; import org.apache.kafka.common.serialization.Serdes; import org.apache.kafka.streams.StreamsBuilder; import org.apache.kafka.streams.StreamsConfig; import org.apache.kafka.streams.Topology; import org.apache.kafka.streams.kstream.KStream; import org.apache.kafka.streams.kstream.KTable; import org.apache.kafka.streams.kstream.Materialized; import org.apache.kafka.streams.kstream.Produced; import java.util.Arrays; import java.util.HashMap; import java.util.Map; public class SimpleStreamsApp { public static void main(final String[] args) { // First, create an environment for executing our Topology.  StreamsExecutionEnvironment env = DefaultStreamsExecutionEnvironment.create(); // Then, define the Kafka Streams configuration,  Map\u0026lt;String, Object\u0026gt; props = new HashMap\u0026lt;\u0026gt;(); props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \u0026#34;localhost:9092\u0026#34;); props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass()); props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass()); // And register the TopologyProvider to the environment.  Conf config = Conf.with(\u0026#34;streams\u0026#34;, props); env.addTopology( WordCountTopologyProvider::new, Executed.as(\u0026#34;wordcount\u0026#34;).withConfig(config) ); // Finally, start the streams environment.  env.start(); } public static class WordCountTopologyProvider implements TopologyProvider { @Override public String version() { return \u0026#34;1.0\u0026#34;; } @Override public Topology get() { final StreamsBuilder builder = new StreamsBuilder(); final KStream\u0026lt;String, String\u0026gt; source = builder.stream(\u0026#34;streams-plaintext-input\u0026#34;); final KTable\u0026lt;String, Long\u0026gt; counts = source .flatMapValues(value -\u0026gt; Arrays.asList(value.toLowerCase().split(\u0026#34;\\\\W+\u0026#34;))) .groupBy((key, value) -\u0026gt; value) .count(Materialized.as(\u0026#34;count\u0026#34;)); counts.toStream().to( \u0026#34;streams-wordcount-output\u0026#34;, Produced.with(Serdes.String(), Serdes.Long()) ); return builder.build(); } } } Before building and running the application, let's make our TopologyProvider a little more configurable.\nMake a TopologyProvider Configurable Currently, the names of the source and sink topics are hard-coded. This is also the case for the materialized state store count. To make our topology configurable, we will modify the WordCountTopologyProvider class to implement the interface io.streamthoughts.azkarra.api.config.Configurable.\nThis interface defines a method configure that accepts an argument of type Conf.\npublic static class WordCountTopologyProvider implements TopologyProvider, Configurable { private String topicSource; private String topicSink; private String stateStoreName; @Override public void configure(final Conf conf) { topicSource = conf.getString(\u0026#34;topic.source\u0026#34;); topicSink = conf.getString(\u0026#34;topic.sink\u0026#34;); stateStoreName = conf.getOptionalString(\u0026#34;state.store.name\u0026#34;).orElse(\u0026#34;count\u0026#34;); } @Override public String version() { return \u0026#34;1.0\u0026#34;; } @Override public Topology get() { final StreamsBuilder builder = new StreamsBuilder(); final KStream\u0026lt;String, String\u0026gt; source = builder.stream(topicSource); final KTable\u0026lt;String, Long\u0026gt; counts = source .flatMapValues(value -\u0026gt; Arrays.asList(value.toLowerCase().split(\u0026#34;\\\\W+\u0026#34;))) .groupBy((key, value) -\u0026gt; value) .count(Materialized.as(stateStoreName)); // need to override value serde to Long type  counts.toStream().to(topicSink, Produced.with(Serdes.String(), Serdes.Long())); return builder.build(); } } Now, we can update the configuration defined above in order to configure our WordCountTopologyProvider class. The StreamsExecutionEnvironment will be responsible for invoking the method configure() before retrieving the Topology.\nFor doing this, we can use the helper class ConfBuilder.\nConf config = ConfBuilder.newConf() .with(\u0026#34;streams\u0026#34;, props) .with(\u0026#34;topic.source\u0026#34;, \u0026#34;streams-plaintext-input\u0026#34;) .with(\u0026#34;topic.sink\u0026#34;, \u0026#34;streams-wordcount-output\u0026#34;) .with(\u0026#34;state.store.name\u0026#34;, \u0026#34;WordCount\u0026#34;) .build(); // And register the TopologyProvider to the environment.  env.addTopology( WordCountTopologyProvider::new, Executed.as(\u0026#34;wordcount\u0026#34;).withConfig(config) ); Instead of configuring our streams and our provider on the topology-level, we can passed the configuration to the StreamsExecutionEnvironment. All topologies will then automatically inherit the configuration of their StreamsExecutionEnvironment.\nStreamsExecutionEnvironment env = DefaultStreamsExecutionEnvironment.create(config); Making a topology configurable is always recommend. This can be useful, for example, if you need to test a topology with different parameters.\nThe complete code so far is this:\npackage azkarra; import io.streamthoughts.azkarra.api.Executed; import io.streamthoughts.azkarra.api.StreamsExecutionEnvironment; import io.streamthoughts.azkarra.api.config.Conf; import io.streamthoughts.azkarra.api.config.ConfBuilder; import io.streamthoughts.azkarra.api.config.Configurable; import io.streamthoughts.azkarra.api.streams.TopologyProvider; import io.streamthoughts.azkarra.runtime.env.DefaultStreamsExecutionEnvironment; import org.apache.kafka.common.serialization.Serdes; import org.apache.kafka.streams.StreamsBuilder; import org.apache.kafka.streams.StreamsConfig; import org.apache.kafka.streams.Topology; import org.apache.kafka.streams.kstream.KStream; import org.apache.kafka.streams.kstream.KTable; import org.apache.kafka.streams.kstream.Materialized; import org.apache.kafka.streams.kstream.Produced; import java.util.Arrays; import java.util.HashMap; import java.util.Map; public class SimpleStreamsApp { public static void main(final String[] args) { // First, define the Kafka Streams configuration,  Map\u0026lt;String, Object\u0026gt; props = new HashMap\u0026lt;\u0026gt;(); props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \u0026#34;localhost:9092\u0026#34;); props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass()); props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass()); final Conf config = ConfBuilder.newConf() .with(\u0026#34;streams\u0026#34;, props) .with(\u0026#34;topic.source\u0026#34;, \u0026#34;streams-plaintext-input\u0026#34;) .with(\u0026#34;topic.sink\u0026#34;, \u0026#34;streams-wordcount-output\u0026#34;) .with(\u0026#34;state.store.name\u0026#34;, \u0026#34;WordCount\u0026#34;) .build(); // Then, create an environment for executing our Topology.  StreamsExecutionEnvironment env = DefaultStreamsExecutionEnvironment.create(config); // And register the TopologyProvider to the environment.  env.addTopology(WordCountTopologyProvider::new, Executed.as(\u0026#34;wordcount\u0026#34;)); // Finally, start the streams environment.  env.start(); } public static class WordCountTopologyProvider implements TopologyProvider , Configurable { private String topicSource; private String topicSink; private String stateStoreName; @Override public void configure(final Conf conf) { topicSource = conf.getString(\u0026#34;topic.source\u0026#34;); topicSink = conf.getString(\u0026#34;topic.sink\u0026#34;); stateStoreName = conf.getOptionalString(\u0026#34;state.store.name\u0026#34;).orElse(\u0026#34;count\u0026#34;); } @Override public String version() { return \u0026#34;1.0\u0026#34;; } @Override public Topology get() { final StreamsBuilder builder = new StreamsBuilder(); final KStream\u0026lt;String, String\u0026gt; source = builder.stream(topicSource); final KTable\u0026lt;String, Long\u0026gt; counts = source .flatMapValues(value -\u0026gt; Arrays.asList(value.toLowerCase().split(\u0026#34;\\\\W+\u0026#34;))) .groupBy((key, value) -\u0026gt; value) .count(Materialized.as(stateStoreName)); counts.toStream().to(topicSink, Produced.with(Serdes.String(), Serdes.Long())); return builder.build(); } } } There is still one missing piece to our application. Indeed, it's always recommend to configure a Java shutdown hook to ensure that our JVM shutdowns are handled gracefully.\nAdding a ShutdownHook For adding a shutdown hook we are going to refactor our application by introducing a new concept called AzkarraContext. An AzkarraContext is used to manage lifecycle of one or more StreamsExecutionEnvironment. Like an environment an AzkarraContext can also be configured and all environments will automatically inherit from it.\nReplace your main() method with the code below :\npublic static void main(final String[] args) { // First, define the Kafka Streams configuration,  Map\u0026lt;String, Object\u0026gt; props = new HashMap\u0026lt;\u0026gt;(); props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \u0026#34;localhost:9092\u0026#34;); props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass()); props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass()); final Conf config = ConfBuilder.newConf() .with(\u0026#34;streams\u0026#34;, props) .with(\u0026#34;topic.source\u0026#34;, \u0026#34;streams-plaintext-input\u0026#34;) .with(\u0026#34;topic.sink\u0026#34;, \u0026#34;streams-wordcount-output\u0026#34;) .with(\u0026#34;state.store.name\u0026#34;, \u0026#34;WordCount\u0026#34;) .build(); // Then, create an environment for executing our Topology.  StreamsExecutionEnvironment env = DefaultStreamsExecutionEnvironment.create(); // And register the TopologyProvider to the environment.  env.addTopology(WordCountTopologyProvider::new, Executed.as(\u0026#34;wordcount\u0026#34;)); // Finally, create and start an Azkarra Context.  DefaultAzkarraContext.create(config) .addExecutionEnvironment(env) .setRegisterShutdownHook(true) .start(); } As you can see, this is the AzkarraContext which is responsible for starting the environment. In addition, we have moved the configuration from the StreamsExecutionEnvironment to the AzkarraContext.\nThe Azkarra API is designed to allow sensible overriding of properties. Configuration are considered in the following order :\n Topology (using the Executed). StreamsExecutionEnvironment (using create() or setConfiguration). AzkarraContext (using create() or setConfiguration).  (Note : You should always have a single AzkarraContext per application even if Azkarra does not enforce a singleton pattern.)\nNow, we are ready to build and run our application.\nRunning You App on Docker Before running this example, you will have to start a Kafka Cluster. For that purpose we will use the official Kafka Docker images maintain by Confluent.Inc.\nYou can start a single-node Kafka Cluster using the filedocker-compose.yml.\n$\u0026gt; cd azkarra-getting-started $\u0026gt; docker-compose up -d Next, we have to create the two source and sink topics used by the topology. For that, you can run the provided script :\n$\u0026gt; chmod u+x ./quickstart-create-wordcount-topics.sh $\u0026gt; ./quickstart-create-wordcount-topics.sh Or directly run those commands :\n$\u0026gt; docker exec -it azkarra-cp-broker /usr/bin/kafka-topics \\ --create --topic streams-plaintext-input --replication-factor 1 --partitions 3 --zookeeper zookeeper:2181 $\u0026gt; docker exec -it azkarra-cp-broker /usr/bin/kafka-topics \\ --create --topic streams-wordcount-output --replication-factor 1 --partitions 3 --zookeeper zookeeper:2181 As a last step, we will package and run the Maven project :\n$\u0026gt; mvn clean package \u0026amp;\u0026amp; java -jar target/azkarra-quickstart-java-0.5.0.jar Let's produce some input messages to Kafka topic streams-plaintext-input :\n$\u0026gt; docker exec -it azkarra-cp-broker /usr/bin/kafka-console-producer \\  --topic streams-plaintext-input --broker-list kafka:9092 Azkarra Streams WordCount I Heart Logs Kafka Streams Making Sense of Stream Processing Then consume from output topic streams-wordcount-output :\n$\u0026gt; docker exec -it azkarra-cp-broker /usr/bin/kafka-console-consumer --from-beginning \\ --property print.key=true --property key.separator=\u0026#34;-\u0026#34; \\  --topic streams-wordcount-output --bootstrap-server kafka:9092 Congratulation! You just run you first KafkaStreams application using Azkarra.\nBut, Azkarra is not limited to that and can do much more. So let’s go ahead and explore what is making Azkarra so cool!\n(In the following section we will omitted the code for the WordCountTopology for clarity)\nSimplifying our Application In the previous example, we have manually created a StreamsExecutionEnvironment and added the WordCountTopologyProvider. Actually, our objective was to give you an overview of Azkarra's main concepts. Defining environments in a programmatic way gives you flexibility but can be sometime cumbersome.\nOur application can be simplified by using a default environment that will be created by the ArkarraContext. For this, we are going to remove the environment instance previously created for directly registering the WordCountTopologyProvider to the context using the method addTopology().\nAzkarraContext context = DefaultAzkarraContext.create(config); context.addTopology(WordCountTopologyProvider.class, Executed.as(\u0026#34;wordcount\u0026#34;)); context.setRegisterShutdownHook(true).start(); Externalizing Context Configuration The externalization of configuration is a requirement to deploy any application in production.\nFor that purpose, Azkarra provide the class so-called AzkarraConf. Internally, it uses the Config library developed by Lightbend to ease the configuration of your application from an external file.\nFirst, we will update the file src/main/resources/application.conf to contain the following code :\nstreams { bootstrap.servers = \u0026quot;localhost:9092\u0026quot; default.key.serde = \u0026quot;org.apache.kafka.common.serialization.Serdes$StringSerde\u0026quot; default.value.serde = \u0026quot;org.apache.kafka.common.serialization.Serdes$StringSerde\u0026quot; } topic { source = \u0026quot;streams-plaintext-input\u0026quot; sink = \u0026quot;streams-wordcount-output\u0026quot; store.name = \u0026quot;WordCount\u0026quot; } Next, we will replace our configuration by loading the application.conf using AzkarraConf.create(\u0026quot;application\u0026quot;)\nThe complete code so far is this:\npackage azkarra; import io.streamthoughts.azkarra.api.Executed; import io.streamthoughts.azkarra.api.config.Conf; import io.streamthoughts.azkarra.api.config.Configurable; import io.streamthoughts.azkarra.api.streams.TopologyProvider; import io.streamthoughts.azkarra.runtime.context.DefaultAzkarraContext; import io.streamthoughts.azkarra.streams.config.AzkarraConf; import org.apache.kafka.common.serialization.Serdes; import org.apache.kafka.streams.StreamsBuilder; import org.apache.kafka.streams.Topology; import org.apache.kafka.streams.kstream.KStream; import org.apache.kafka.streams.kstream.KTable; import org.apache.kafka.streams.kstream.Materialized; import org.apache.kafka.streams.kstream.Produced; import java.util.Arrays; public class SimpleStreamsApp { public static void main(final String[] args) { // Create a new AzkarraConf from the classpath file application.conf  AzkarraConf config = AzkarraConf.create(); // Create and start a default AzkarraContext  AzkarraContext context = DefaultAzkarraContext.create(config); context.addTopology(WordCountTopologyProvider.class, Executed.as(\u0026#34;wordcount\u0026#34;)); context.setRegisterShutdownHook(true).start(); } public static class WordCountTopologyProvider implements TopologyProvider , Configurable { // code omitted for clarity....  } } Auto-Configure Azkarra supports a mechanism of auto-configuration for automatically declaring environments and topologies to be run. For that, we will introduce a new concept so-called AzkarraApplication.\nMost of the time, the AzkarraApplication will be your main entry point when creating a new streams application. Basically, this class is responsible for initializing and managing your application based on the specified configuration. Internally, it will create an AzkarraContext instance and the StreamsExecutionEnvrionment instances declared in your configuration.\npackage azkarra; import io.streamthoughts.azkarra.api.config.Conf; import io.streamthoughts.azkarra.api.config.Configurable; import io.streamthoughts.azkarra.api.streams.TopologyProvider; import io.streamthoughts.azkarra.streams.AzkarraApplication; import io.streamthoughts.azkarra.streams.config.AzkarraConf; import org.apache.kafka.common.serialization.Serdes; import org.apache.kafka.streams.StreamsBuilder; import org.apache.kafka.streams.Topology; import org.apache.kafka.streams.kstream.KStream; import org.apache.kafka.streams.kstream.KTable; import org.apache.kafka.streams.kstream.Materialized; import org.apache.kafka.streams.kstream.Produced; import java.util.Arrays; public class SimpleStreamsApp { public static void main(final String[] args) { // Create configuration from externalized file.  AzkarraConf config = AzkarraConf.create(); new AzkarraApplication(SimpleStreamsApp.class) .setConfiguration(config) .setRegisterShutdownHook(true) .run(args); } public static class WordCountTopologyProvider implements TopologyProvider , Configurable { // code omitted for clarity....  } } Next, update the application.conf to contain the following code :\nNote that we have to create a parent config named azkarra.\nazkarra { // The context configuration. context { streams { bootstrap.servers = \u0026quot;localhost:9092\u0026quot; default.key.serde = \u0026quot;org.apache.kafka.common.serialization.Serdes$StringSerde\u0026quot; default.value.serde = \u0026quot;org.apache.kafka.common.serialization.Serdes$StringSerde\u0026quot; } } // List of components to be registered into the AzkarraContext (e.g : TopologyProviders). components = [ \u0026quot;azkarra.SimpleStreamsApp$WordCountTopologyProvider\u0026quot; ] // List of environments. environments = [ { name = \u0026quot;__default\u0026quot; config = {} // List of streams jobs to execute jobs = [ { name = \u0026quot;word-count-demo\u0026quot; description = \u0026quot;Kafka Streams WordCount Demo\u0026quot; topology = WordCountTopology // The topology configuration. config { topic { source = \u0026quot;streams-plaintext-input\u0026quot; sink = \u0026quot;streams-wordcount-output\u0026quot; store.name = \u0026quot;WordCount\u0026quot; } } } ] } ] } Embedded Http Server Azkarra ships with an embedded non-blocking HTTP server (based on Undertow) to expose RESTful APIs. These APIs can be used not only to manage your local Kafka Streams instances and Topologies but also to execute interactive queries on your application states stores.\nThe HTTP-Server can be enable and configured via the AzkarraApplication as follows :\nAzkarraConf config = AzkarraConf.create(); new AzkarraApplication(SimpleStreamsApp.class) .setConfiguration(config) // Enable http server and set the context.  .enableHttpServer(true, HttpServerConf.with(\u0026#34;localhost\u0026#34;, 8080)) .setRegisterShutdownHook(true) .run(args); Let's have a look to some REST APIs.\n List local streams instances :  $\u0026gt; curl -sX GET \u0026#39;http://localhost:8080/api/v1/streams\u0026#39; | jq  Get details about a running streams instance  $\u0026gt; curl -sX GET \u0026#39;http://localhost:8080/api/v1/streams/word-count-demo-1-0\u0026#39; | jq  Get metrics of a running streams instance  $\u0026gt; curl -sX GET \u0026#39;http://localhost:8080/api/v1/streams/word-count-demo-1-0/metrics\u0026#39; | jq  Query state store  $\u0026gt; curl -sX POST \u0026#39;http://localhost:8080/api/v1/applications/word-count-demo-1-0/stores/count\u0026#39; --data \u0026#39;{ \u0026#34;type\u0026#34;:\u0026#34;key_value\u0026#34;, \u0026#34;query\u0026#34; : {\u0026#34;all\u0026#34;:{} } }\u0026#39; | jq Annotations and Component Scan For even more simplicity, Azkarra provides mechanisms for auto-discovering TopologyProvider classes.\nFinally, the complete version of our application looks like this :\npackage azkarra; import io.streamthoughts.azkarra.api.annotations.Component; import io.streamthoughts.azkarra.api.config.Conf; import io.streamthoughts.azkarra.api.config.Configurable; import io.streamthoughts.azkarra.api.streams.TopologyProvider; import io.streamthoughts.azkarra.streams.AzkarraApplication; import io.streamthoughts.azkarra.streams.autoconfigure.annotations.AzkarraStreamsApplication; import org.apache.kafka.common.serialization.Serdes; import org.apache.kafka.streams.StreamsBuilder; import org.apache.kafka.streams.Topology; import org.apache.kafka.streams.kstream.KStream; import org.apache.kafka.streams.kstream.KTable; import org.apache.kafka.streams.kstream.Materialized; import org.apache.kafka.streams.kstream.Produced; import java.util.Arrays; @AzkarraStreamsApplication public class SimpleStreamsApp { public static void main(final String[] args) { AzkarraApplication.run(SimpleStreamsApp.class, args); } @Component public static class WordCountTopologyProvider implements TopologyProvider , Configurable { private String topicSource; private String topicSink; private String stateStoreName; @Override public void configure(final Conf conf) { topicSource = conf.getString(\u0026#34;topic.source\u0026#34;); topicSink = conf.getString(\u0026#34;topic.sink\u0026#34;); stateStoreName = conf.getOptionalString(\u0026#34;state.store.name\u0026#34;).orElse(\u0026#34;count\u0026#34;); } @Override public String version() { return \u0026#34;0.1\u0026#34;; } @Override public Topology get() { final StreamsBuilder builder = new StreamsBuilder(); final KStream\u0026lt;String, String\u0026gt; source = builder.stream(topicSource); final KTable\u0026lt;String, Long\u0026gt; counts = source .flatMapValues(value -\u0026gt; Arrays.asList(value.toLowerCase().split(\u0026#34;\\\\W+\u0026#34;))) .groupBy((key, value) -\u0026gt; value) .count(Materialized.as(stateStoreName)); counts.toStream().to(topicSink, Produced.with(Serdes.String(), Serdes.Long())); return builder.build(); } } } The @AzkarraStreamsApplication is a convenience annotation that adds all of the following:\n @EnableAutoConfig: Tells Azkarra to auto-configure internal AzkarraContext @EnableAutoStart: Tells Azkarra to automatically start any registered TopologyProvider using the default environment. @EnableEmbeddedHttpServer: Tells Azkarra to start the embedded http-server. @ComponentScan: Tells Azkarra to look for classes annotated with @Component in the current package.  We can simplify the application.conf by removing the environments sections. Azkarra will automatically load all your TopologyProvider by scanning your project classpath.\nNow, the application.conf should only contain the following code :\nazkarra { // The context configuration. context { streams { bootstrap.servers = \u0026quot;localhost:9092\u0026quot; default.key.serde = \u0026quot;org.apache.kafka.common.serialization.Serdes$StringSerde\u0026quot; default.value.serde = \u0026quot;org.apache.kafka.common.serialization.Serdes$StringSerde\u0026quot; } topic { source = \u0026quot;streams-plaintext-input\u0026quot; sink = \u0026quot;streams-wordcount-output\u0026quot; store.name = \u0026quot;WordCount\u0026quot; } } } Azkarra Web UI And, finally, we saved the best for last! Azkarra also provide a default WebUI for exploring your local streams application. Azkarra Streams Web UI\n","excerpt":"In this tutorial we will explore the main Azkarra APIs step by step. For that purpose we are going …","ref":"https://streamthoughts.github.io/azkarra-streams/docs/getting-started/","title":"Getting Started"},{"body":"This section describes how you can configure an Azkarra Application.\nConfiguring an Azkarra Application The AzkarraApplication is responsible for configuring, and assembling the AzkarraContext, StreamsExecutionEnvironment and TopologyProvider instances.\nFor doing that, the AzkarraApplication is expecting specific properties :\nContext:\n   Property Type Description     azkarra.context config The configuration of the AzkarraContext   azkarra.components Array[string] The list of components (e.g: TopologyProvider) to be registered into the AzkarraContext.   azkarra.environments Array[Environment] The list of StreamsExecutionEnvironment to initialize and configure.   azkarra.server config The configuration of the embedded HTTP server.    Environment:\n   Property Type Description     name string The environment name.   config config The environment configuration.   jobs Array[Job] The List of Kafka Streams jobs to run.    Streams/Topologies :\n   Property Type Description     name string A short name which is used for identifying the streams job.   description string A optional description of the streams job.   topology string The fully qualified class name of the topology or an alias.   config config The props used to configure both KafkaStreams and TopologyProvider instances.    Complete example (using HOCON format):\nazkarra { // The context configuration context { // The default configuration for streams application. streams { bootstrap.servers = \u0026quot;localhost:9092\u0026quot; default.key.serde = \u0026quot;org.apache.kafka.common.serialization.Serdes$StringSerde\u0026quot; default.value.serde = \u0026quot;org.apache.kafka.common.serialization.Serdes$StringSerde\u0026quot; } } // Manually defines the providers to be registered components = [ io.streamthoughts.azkarra.example.topology.WordCountTopology ] // Create a default environment for running the WordCountTopology environments = [ { name: \u0026quot;__default\u0026quot; jobs = [ { name = \u0026quot;word-count-demo\u0026quot; description = \u0026quot;Kafka Streams WordCount Demo\u0026quot; topology = \u0026quot;WordCountTopology\u0026quot; config = {} } ] } ] server { listener = localhost port = 8080 headless = false // These information will be exposes through the http endpoint GET /info info { app { name = \u0026quot;@project.name@\u0026quot; description = \u0026quot;@project.description@\u0026quot; version = \u0026quot;@project.version@\u0026quot; encoding = \u0026quot;@project.build.sourceEncoding@\u0026quot; java.version = \u0026quot;@java.version@\u0026quot; } } } } Code snippet for auto-configuring an AzkarraApplication :\nvar appConfig = ...; var application = new AzkarraApplication(); application.configure(appConfig); Conf Implementations The Azkarra project provides some built-in Conf implementations which are internally used.\n ArgsConf : A Conf implementation that can be created from an arguments array. AzkarraConf : A Conf implementation which is based on the Lightbend Config library. MapConf : A Conf implementation that can be created from a Map. Property : A single key-value with configuration.  The Conf interface define methods : getConfAsMap and getConfAsProperties which be can used to easily creating a Consumer/Producer/KafkaStreams instance.\nConfiguring components Azkarra allows any component registered into the AzkarraContext as well as topology providers to be configurable. For that, a component have to implement the interface io.streamthoughts.azkarra.api.config.Configurable :\npublic interface Configurable { void configure(final Conf configuration); } For components returned from method AzkarraContext#getComponent(), the method configure is automatically invoke after instantiating the component. The Conf instance passed as parameter is the AzkarraContext configuration.\nFor TopologyProvider, the configure method is automatically invoked after instantiating the component. The Conf instance passed as parameter will contain the StreamsExecutionEnvironment configuration merged with the configuration provided through the Executed object.\nIn addition, a TopologyProvider that wishes to be notified of the StreamsExecutionEnvironment that it runs in can implement the StreamsExecutionEnvironmentAware.\nThis can makes sense for example when an provider requires access to the environment name or configuration.\nComponent Scan Azkarra provides a simple mechanism to search for components present in the classpath. Any class annotated with @Component or implementing the interface ComponentFactory will be scanned and automatically registered to the AzkarraContext.\nThe component scan can be enable programmatically through the method AzkarraApplication#setEnableComponentScan. To automatically start all registered TopologyProviver, you must also enable auto-start.\nCode snippet for enabling component scan :\napplication .setEnableComponentScan(true) .setAutoStart(true) Annotation Based Configuration Another way to initialize and configure an AzkarraApplication is to use annotations. Therefore, you can simply annotated you main class with @AzkarraStreamsApplication and initialize your AzkarraApplication as follows:\n@AzkarraStreamsApplication public class SimpleStreamsApp { public static void main(final String[] args) { AzkarraApplication.run(SimpleStreamsApp.class, args); } } The @AzkarraStreamsApplication is a convenience annotation that adds all of the following:\n @EnableAutoConfig: Tells Azkarra to auto-configure internal AzkarraContext @EnableAutoStart: Tells Azkarra to automatically start any registered TopologyProvider using the default environment. @EnableEmbeddedHttpServer: Tells Azkarra to start the embedded http-server. @ComponentScan: Tells Azkarra to look for classes annotated with @Component in the current package.  In addition, Azkarra allows TopologyProvider to be annotated for providing default metadata and streams configuration.\nCurrently, TopologyProvider supports the following annotations :\n @TopologyInfo: Set the topology description and custom alias. @DefaultStreamsConfig: Set a default streams configuration (repeatable).  You should note that annotations are only supported for topologies which are registered into an AzkarraContext.\nExample\n@TopologyInfo( description = \u0026#34;Kafka Streams WordCount Demo\u0026#34;, aliases = \u0026#34;custom\u0026#34;) @DefaultStreamsConfig(name = StreamsConfig.NUM_STREAM_THREADS_CONFIG, value = \u0026#34;4\u0026#34;) @DefaultStreamsConfig(name = StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, value = \u0026#34;2\u0026#34;) class MyCustomTopologyProvider implements TopologyProvider, Configureable { public void configure(final Conf conf) { ... } public Topology get() { return ...; } } ","excerpt":"This section describes how you can configure an Azkarra Application.\nConfiguring an Azkarra …","ref":"https://streamthoughts.github.io/azkarra-streams/docs/developer-guide/configuration/","title":"Configuration"},{"body":"The dependency injection pattern is a common practice in software development to get code that is modular and testable.\nAzkarra does not implement any advanced dependency injection mechanism. We think that you can leverage one of the existing IoC frameworks for achieving this.\nHowever, Azkarra implements a simple mechanism based on the Supplier interface for internally registering and getting concrete class instances.\nYou can easily use this mechanism to wire your services together.\nDefining Components When component-scan is enable, Azkarra will scan the current classpath and the configured external paths (i.e: azkarra.component.paths) to look up for classes with @Component or @Factory annotations.\nYou can enable component-scan either programmatically :\nnew AzkarraApplication().setEnableComponentScan(true) or by annotate your main class with one of these annotations: @AzkarraStreamsApplication or @ComponentScan.\nThe following is a simple component example:\n@Component @Singleton public class ConfigurableStopWordsService implements StopWordsService, Configurable { private List\u0026lt;String\u0026gt; stopsWords; @Override public void configure(final Conf conf) { stopsWords = conf.getStringList(\u0026#34;service.stopwords\u0026#34;); } @Override public boolean test(final String word) { return stopsWords.contains(word.toLowerCase()); } } Azkarra does not implement the JSR-330 (javax.inject) - Dependency Injection for Java specification but leverages some of the defined Java annotations :\n Singleton Named  Components are automatically registered to the AzkrarraContext which implement the interface ComponentRegistry.\nComponents can also be register programmatically :\nAzkarraContext context = ... context.registerComponent(ConfigurableStopWordsService.class); or\ncontext.registerSingleton(ConfigurableStopWordsService.class); Note : A component is registered both for its type and all its supertypes.\nA component instance can be retrieve directly from the AzkarraContext or the ComponentFactory.\nStopWordsService service = context.getComponent(StopWordsService.class); The returned instance may be shared or independent, depending if it has been registered as a singleton or prototype. In addition, if the component implements Configurable, then the new instance will be automatically configured using the context configuration.\nInternally, Azkarra uses the ComponentFactory not only to automatically discover available Kafka Streams topologies but also any classes that implement :\n StreamsLifecycleInterceptor KafkaStreamsFactory HealthIndicator  This allows developers to easily extend Azkarra Streams features.\nQualifying By Name By default, a component is named based on the name of its class.\nTo explicitly define the name of the component, you can use the Named annotation at the class level.\n@Component @Singleton @Named(\u0026#34;StopWordsService\u0026#34;) public class ConfigurableStopWordsService implements StopWordsService, Configurable { private List\u0026lt;String\u0026gt; stopsWords; @Override public void configure(final Conf conf) { stopsWords = conf.getStringList(\u0026#34;service.stopwords\u0026#34;); } @Override public boolean test(final String word) { return stopsWords.contains(word.toLowerCase()); } } StopWordsService service = context.getComponent(StopWordsService.class, Qualifiers.byName(\u0026#34;StopWordsService\u0026#34;)); Component Version A component is uniquely identify by a type, a name and optionally a version.\nA component class that implements the Versioned interface can be qualified by the version return from the Versioned:version method.\nFor example, this is the case of the TopologyProvider interface.\npublic interface Versioned { String version(); } Currently, Azkarra supports the following version schema:\n\u0026lt;major version\u0026gt;.\u0026lt;minor version\u0026gt;.\u0026lt;incremental version\u0026gt;-\u0026lt;qualifier\u0026gt;.\nHere are some valid examples :\n 2 2.0 2.0.1 2.0.1-SNAPSHOT 2.0.1-rc3  Moreover, Azkarra only supports a pre-defined subset of qualifiers: alpha, beta, snapshot, rc and release.\nYou can then get a component instance for a specific version.\nWordCountTopology topology = context.getComponent(WordCountTopology.class, Qualifiers.byVersion(\u0026#34;1.0.0\u0026#34;)); or\nWordCountTopology topology = context.getComponent(WordCountTopology.class, Qualifiers.byLatestVersion()); Note: A class that implement the Versionned interface must define a no-arg constructor.\nComponent Factories Defining a component either programmatically or using the @Component annotation is pretty straightforward. However, this approach has some limitations. First, a class annotated with the @Component annotation must have a no-arg constructor in order to be instantiable. Secondly, because you cannot annotated classes provided by third-party libraries, you cannot use the @Component annotation to register as a component a class that is not part of your codebase.\nFor that, you can use a factory class.\nA factory class is a simple Java class annotated with the @Factory annotation that provides one or more methods annotated with @Component.\n@Factory public class TopicsFactory { @Component @Singleton public NewTopic streamsInputTopic() { return new NewTopic(\u0026#34;streams-plaintext-input\u0026#34;, 6, (short)1); } @Component @Singleton public NewTopic streamsOuputTopic() { return new NewTopic(\u0026#34;streams-wordcount-output\u0026#34;, 6, (short)1); } } For each method, Azkarra will create one proxy Supplier instance that delegate the component creation to one instance of the TopicsFactory.\nNote : A factory class can implement the Configurable interface.\nComponent Suppliers Another way to provide a component is to directly annotated a class implementing the Supplier interface with the @Component annotation.\n@Component @Bean(\u0026#34;simpleStopWordsService\u0026#34;) public class StopWordsServiceSupplier implements Supplier\u0026lt;StopWordsService\u0026gt;, Configurable { private Conf conf; @Override public void configure(final Conf conf) { this.conf = conf; } @Override public StopWordsService get() { return new SimpleStopWordsService(conf.getStringList(\u0026#34;service.stopwords\u0026#34;)); } } Building the Graph Azkarra does NOT support the @Inject annotation specified by JSR-330 to automatically linked components by their dependencies.\nDevelopers are responsible for building the graph of objects programmatically by defining either a @Factory class or a Supplier that implement the ComponentFactoryAware interface and Configurable (optionally).\nFor that purpose Azkarra provides the ComponentModule class.\nThe following is a simple example:\n@Component public class ComplexWordCountTopologyModule extends ComponentModule\u0026lt;ComplexWordCountTopology\u0026gt; { @Override public ComplexWordCountTopology get() { StopWordsService service = getComponent(StopWordsService.class); ComplexWordCountTopology topology = new ComplexWordCountTopology(); topology.setStopWordsService(service); return topology; } } Restricted Component The DefaultAzkarraContext class will try to automatically configure streams environments and streams topologies using the registered components. Specifically, the context looks for the components of type :\n StreamsLifecycleInterceptor KafkaStreamsFactory NewTopic (when auto.create.topics.enable is true)  In many cases, you may want the DefaultAzkarraContext to only configure such component for specific environments or streams topologies.\nAzkarra defines the @Restricted annotation that can be used to limit/qualify the usage scope of a component.\nA component can be restricted to :\n one ore many specific streams environments (type = env). one ore many specific streams topologies (type = streams). the application (type = application).  For example, the following KafkaStreamsFactory will only be used for the streams application named wordCountTopology.\n@Component @Restricted(type = Restriction.TYPE_STREAMS, names = \u0026#34;wordCountTopology\u0026#34;) public static class CustomKafkaStreamsFactory implements KafkaStreamsFactory { @Override public KafkaStreams make(final Topology topology, final Conf streamsConfig) { return new KafkaStreams(topology, streamsConfig.getConfAsProperties()); } } Note: Defining a component with @Restricted(type = Restriction.TYPE_APPLICATION) annotation is equivalent to no annotation.\nA complete code example is available on the GitHub project azkarra-examples\n","excerpt":"The dependency injection pattern is a common practice in software development to get code that is …","ref":"https://streamthoughts.github.io/azkarra-streams/docs/developer-guide/dependency-injection/","title":"Dependency Injection"},{"body":"The Developer Guide section helps you learn about the functionalities of the Azkarra Streams Framework and the concepts Azkarra uses to execute your Kafka Streams instance, and helps you obtain a deeper understanding of how Azkarra Streams works.\n","excerpt":"The Developer Guide section helps you learn about the functionalities of the Azkarra Streams …","ref":"https://streamthoughts.github.io/azkarra-streams/docs/developer-guide/","title":"Developer Guide"},{"body":"Azkarra maintains an intercepting filter chain internally to easily perform operations while starting or stopping a Kafka Streams instances by implementing and registering StreamsLifecycleInterceptor instances.\nAzkarra provides built-in interceptors for common operations like waiting for topics to be created before starting a streams instance.\nThe StreamsLifecycleInterceptor Interface The StreamsLifecycleInterceptor interface defines two methods onStart and onStop that are respectively invoked before the streams instance is started or before is stopped.\nYou should always take care to call chain.execute() to not break the chain.\npublic interface StreamsLifecycleInterceptor { /** * Intercepts the streams instance before being started. */ default void onStart(final StreamsLifecycleContext context, final StreamsLifecycleChain chain) { // code here is executed before the streams is being started.  chain.execute(); // code here is executed after the streams was started (successfully or not).  } /** * Intercepts the streams instance before being stopped. */ default void onStop(final StreamsLifecycleContext context, final StreamsLifecycleChain chain) { // code here is executed before the streams is being stopped.  chain.execute(); // code here is executed after the streams was stopped.  } } The information about the current streams application, such as the application ID or the topology description, can be retrieved from the StreamsLifecycleContext argument. The StreamsLifecycleContext object can also be used for updating the current state of the Kafka Streams instance.\nRegistering an Interceptor StreamsLifecycleInterceptor can be registered like any other components using the registerComponent methods that are exposed by the AzkarraContext class or dynamically using the component-scan mechanism. The AzkarraContext will be responsible to add the registered interceptors to the StreamsExecutionEnvironments and topologies. `\nThe interceptors can also be directly add on a StreamsExecutionEnvironment level using the addStreamsLifecycleInterceptor method. When, an interceptor is add to an environment, then it will be executed for all topologies running in that environment.\nenv.addStreamsLifecycleInterceptor(() -\u0026gt; new MyCustomInterceptor()); Finally, interceptors can be defined per topology through the used of the Executed#withInterceptor method.\nenv.addTopology( ()-\u0026gt; new WordCountTopology(), Executed.as(\u0026#34;wordcount\u0026#34;).withInterceptor(() -\u0026gt; new MyCustomInterceptor()) ); Configuring an Interceptor Like any other component, a StreamLifecycleInterceptor can implement the Configurable interface. The Conf object passed to the configure() method corresponds to the topology configuration.\nWaitForSourceTopicsInterceptor When starting a new KafkaStreams instance, the application will fail while performing tasks assignment if one of the source topic is missing (error: INCOMPLETE_SOURCE_TOPIC_METADATA).\nTo prevent from such error, Azkarra provides the built-in WaitForSourceTopicsInterceptor that block the KafkaStreams startup until all source topics are created.\nThe WaitForSourceTopicsInterceptor can be enable by setting the global application property azkarra.context.enable.wait.for.topics to true in your application.conf file.\nIn addition, you can enable that interceptor per environment using the StreamsExecutionEnvironment#setWaitForTopicsToBeCreated method.\nAutoCreateTopicsInterceptor During the development phase, you may find yourself creating and deleting Kafka topics manually and before each run of your application. To ease this operation, Azkarra provides the built-in AutoCreateTopicsInterceptor which can be used to automatically create the source and sink topics before the streams application is started.\nWhen enabled, the AutoCreateTopicsInterceptor is automatically configured by the AzkarraContext. The AzkarraContext will use the following properties to configure the AutoCreateTopicsInterceptor.\n   Property Type Description     auto.create.topics.enable boolean If true, creates all source and sink topics used by the topology.   auto.create.topics.num.partitions int The default number of partition.   auto.create.topics.replication.factor| int The default replication factor.    auto.create.topics.configs Map[string, string] The configuration to be used for creating topics.    You can also add and configure a AutoCreateTopicsInterceptor to a StreamsExecutionEnvironment instance : Here is a simple example :\nStreamsExecutionEnvironment env = DefaultStreamsExecutionEnvironment.create(); env.addStreamsLifecycleInterceptor( () -\u0026gt; { AutoCreateTopicsInterceptor interceptor = new AutoCreateTopicsInterceptor(); interceptor.setReplicationFactor((short)3); interceptor.setNumPartitions(6); return interceptor; }); Defining the list of Topics By default, the AutoCreateTopicsInterceptor resolves the list of topics to be created from the TopologyDescription object. But, you can also specify your own list of NewTopic to be created.\nenv.addStreamsLifecycleInterceptor( () -\u0026gt; { AutoCreateTopicsInterceptor interceptor = new AutoCreateTopicsInterceptor(); interceptor.setTopics(Collections.singletonList( new NewTopic(\u0026#34;my-source-topic\u0026#34;, 6, (short)3)) ); return interceptor; }); When, the AutoCreateTopicsInterceptor is enable on context-level, the AzkarraContext will lookup for registered components of type NewTopic. If you run multiple streams topologies (or environments) you can use the @Restricted annotation to specify the target environment or streams of the component.\nHere is a simple example :\n@Factory public class TopicsFactory { @Component @Restricted(type = \u0026#34;streams\u0026#34;, names = \u0026#34;wordCountTopology\u0026#34;) public NewTopic sourceTopic() { return new NewTopic(\u0026#34;my-source-topic\u0026#34;, 6, (short)3); } } Automatically deleting topics The AutoCreateTopicsInterceptor can also be used for automatically deleting any topics used by the topology when the streams instance is stopped. Note: This property should be used with care and not enable for production.\n   Property Type Description     auto.delete.topics.enable boolean If true, deletes all topics after the streams is stopped (should only be used for development)    ","excerpt":"Azkarra maintains an intercepting filter chain internally to easily perform operations while …","ref":"https://streamthoughts.github.io/azkarra-streams/docs/developer-guide/streamslifecycle-interceptors/","title":"The StreamsLifeCycle Intercepting Chain"},{"body":"Example: Basic TopologyProvider Example: Configurable TopologyProvider Example: Writing a TopologyProvider as a Module Example: Building an Azkarra Application without component-scan Example: Writing a custom HealthCheck Example: Configuring an azkarra application with Basic authentication Example: Configuring an azkarra application with SSL-Two-Way authentication ","excerpt":"Example: Basic TopologyProvider Example: Configurable TopologyProvider Example: Writing a …","ref":"https://streamthoughts.github.io/azkarra-streams/docs/examples/","title":"Examples"},{"body":"Downloading Azkarra Worker AZKARRA_VERSION=0.5.0 wget https://github.com/streamthoughts/azkarra-streams/releases/download/v$AZKARRA_VERSION/azkarra-worker-$AZKARRA_VERSION.tar.gz -P . tar -xzvf azkarra-worker-$AZKARRA_VERSION.tar.gz cd azkarra-worker-$AZKARRA_VERSION The installation contains the following structure :\n├── bin │ ├── azkarra-streams-start.sh │ └── azkarra-streams-stop.sh ├── etc │ ├── azkarra.conf │ └── log4j2.xml ├── LICENSE ├── README.md └── share └── java └── azkarra-worker Running Worker You can start a worker process with the following command :\n./bin/azkarra-streams-start.sh [-daemon] Configuring Worker Azkarra uses the Config library developed by Lightbend to ease the configuration of your application from an external file.\nYou can use the -Dconfig.file system property to specify the config source to load.\nExample:\nSTREAMS_JVM_OPTS=-Dconfig.file=./config/azkarra.conf ./bin/azkarra-streams-start.sh Configs can also be passed passed to the command should be in the form of [--property value]*. These parameters are then passed to the Azkarra main method.\nExample:\nSTREAMS_JVM_OPTS=-Dconfig.file=./config/azkarra.conf ./bin/azkarra-streams-start.sh \\ --azkarra.component.paths /usr/share/azkarra-components --azkarra.context.streams.bootstrap.servers localhost:9092 External Components To make your external components (e.g, TopologyProvider) available to the worker, installed them into one or many local directories.\nUse the azkarra.component.paths property to configure the list of locations (separated by a comma) from which the components will be scanned.\nEach configured directories may contain:\n an uber JAR containing all of the classes and third-party dependencies for the component (e.g., topology). a directory containing all JARs for the component.  Using Docker An official Docker image is available on Docker Hub.\nExample :\ndocker run --net host streamthoughts/azkarra-streams-worker \\ --mount type=bind,src=/tmp/azkarra/application.conf,dst=/etc/azkarra/azkarra.conf \\ --mount type=bind,src=/tmp/components,dst=/usr/share/azkarra-components/ \\ streamthoughts/azkarra-streams-worker ","excerpt":"Downloading Azkarra Worker AZKARRA_VERSION=0.5.0 wget …","ref":"https://streamthoughts.github.io/azkarra-streams/docs/azkarra-worker/","title":"Azkarra Worker"},{"body":"By default, the Azkarra API is responsible for creating a new KafkaStreams instance for each provided Topology.\nBut, in some cases, you may want to be able to customize how the KafkaStreams instances are created. For example, it may be to provide a KafkaClientSupplier that will add some tracing mechanisms on top of the Kafka clients (e.g: kafka-opentracing\nYou can implement the KafkaStreamsFactory interface for that.\nBelow is the interface :\npublic interface KafkaStreamsFactory { KafkaStreams make(final Topology topology, final Conf streamsConfig); } If you wish to configure a KafkaStreamsFactory for a specific environment, then you can use the StreamsExecutionEnvironment#setKafkaStreamsFactory method.\nStreamsExecutionEnvironment env = DefaultStreamsExecutionEnvironment.create(); env.setKafkaStreamsFactory(() -\u0026gt; new KafkaStreamsFactory() { @Override public KafkaStreams make(final Topology topology, final Conf streamsConfig) { KafkaClientSupplier clientSupplier = //...  return new KafkaStreams(topology, streamsConfig.getConfAsProperties(), clientSupplier); } }); In addition, a KafkaStreamsFactory can also be provided as a context component.\n@Component public CustomKafkaStreamsFactory implements KafkaStreamsFactory { @Override public KafkaStreams make(final Topology topology, final Conf streamsConfig) { KafkaClientSupplier clientSupplier = //...  return new KafkaStreams(topology, streamsConfig.getConfAsProperties(), clientSupplier); } } Like any other component, a KafkaStreamsFactory can implement the Configurable interface. The Conf object passed to the configure() method corresponds to the topology configuration.\n","excerpt":"By default, the Azkarra API is responsible for creating a new KafkaStreams instance for each …","ref":"https://streamthoughts.github.io/azkarra-streams/docs/developer-guide/kafkastreams-factory/","title":"KafkaStreams Factory"},{"body":"","excerpt":"","ref":"https://streamthoughts.github.io/azkarra-streams/docs/developer-guide/error-management/","title":"Error Management"},{"body":"Internally, KafkaStreams relies on an embedded key-value store so-called RocksDB to provided persistent storage. Depending of the throughput of your application, you may want to tune internal RocksDB instances.\nKafka Streams allows you to customize the RocksDB settings for a given Store by implementing the interface org.apache.kafka.streams.state.RocksDBConfigSetter.\nThe custom implementation must then be configured using :\nstreamsConfig.put(StreamsConfig.ROCKSDB_CONFIG_SETTER_CLASS_CONFIG, CustomRocksDBConfig.class) Azkarra Streams provides a built-in DefaultRocksDBConfigSetter which is automatically configured if no one is already provided.\nDefaultRocksDBConfigSetter allows you to override not only some default RocksDB options but also to enable log statistics for performance debugging.\nAvailable properties are :\n   Property Type Description     rocksdb.stats.enable boolean Enable RocksDB statistics   rocksdb.stats.dump.period.sec integer The RocksDB statistics dump period in seconds.   rocksdb.log.dir string The RocksDB log directory   rocksdb.log.level string The RocksDB log level (see org.rocksdb.InfoLogLevel).   rocksdb.log.max.file.size integer The RocksDB maximum log file size.   rocksdb.max.write.buffer.number integer The maximum number of memtables build up in memory, before they flush to SST files.   rocksdb.write.buffer.size long The size of a single memtable.    Note that all properties described above are optional.\nRocksDB properties can be passed either using default configuration :\ncontext { streams { rocksdb { stats.enable = false stats.dumpPeriodSec = 30 log { dir = \u0026quot;/var/log/kafka-streams/rocksdb\u0026quot; file.size = 104857600 } } } } or programmatically through the method StreamsExecutionEnvironment#setRocksDBConfig:\ncontext.defaultExecutionEnvironment() .setRocksDBSettings( RocksDBConfig.withStatsEnable() .withLogDir(\u0026#34;/tmp/rocksdb-logs\u0026#34;) ); Please read official documentation for more information: RocksDB Tuning Guide\n","excerpt":"Internally, KafkaStreams relies on an embedded key-value store so-called RocksDB to provided …","ref":"https://streamthoughts.github.io/azkarra-streams/docs/developer-guide/rocksdb/","title":"RocksDB"},{"body":"Why Azkarra needs to be secured? Security is one of the most important concerns (if not the most important) when it comes to deploying an application in production.  Azkarra allows you to access information about the running Kafka Streams instances (metrics, configuration, states, etc) via the REST APIs. Some non-critical data, accessible through these HTTP endpoints, may provide information about your application considered to be private. For example, it could be the Kafka topics that are consumed or used internally by your application.\nIn addition, the state stores of your streams application can be queried. Depending on your business nature, you may want to protect some sensitive data returned by the application.\nFinally, the REST APIs enable you to perform some operations like stopping and restarting a Kafka Streams instance. While some of these operations can be useful during development or for some production scenarios, it can be also necessary to not allow them.\nAzkarra is composed of three standard components for security :\n Encryption on the wire: This allows data transmitted between an Azkarra application and a client application or between two Azkarra applications (i.e when using interactive queries) to be encrypted over the network. Authentication: This allows client applications or an Azkarra application, accessing the REST APIs, to authenticate to the Azkarra application. Authorization: This allows you to allow or deny your client applications to perform some operations or to access specific REST resources.  Headless Mode Azkarra has a headless mode that can be used to restrict REST APIs usage.\nWhen the headless mode is enabled, Azkarra returns a 401 Unauthorized HTTP response for POST, PUT and DELETE requests.\nThe only exception to this rule is that you can still query the state stores of your application.\n   Property Type Description     azkarra.server.headless boolean Enable Server Headless mode    Disable Web UI As of Azkarra 0.4, you can also disable the Web UI by using the server configuration :\n   Property Type Description     azkarra.server.enable.ui boolean Enable Web UI    Encryption using TLS (or SSL) azkarra.server.ssl.enable=true azkarra.server.ssl.keystore.location=\u0026quot;/path/to/server.ks.pkcs12\u0026quot; azkarra.server.ssl.keystore.type=\u0026quot;PKCS12\u0026quot; azkarra.server.ssl.keystore=\u0026quot;password\u0026quot; azkarra.server.ssl.key.password=\u0026quot;password\u0026quot; azkarra.server.ssl.truststore.location=\u0026quot;/path/to/server.ts.pkcs12\u0026quot; azkarra.server.ssl.truststore.type=\u0026quot;PKCS12\u0026quot; azkarra.server.ssl.truststore=\u0026quot;password\u0026quot; SSL hostname verification can be disable using the configuration :\nazkarra.server.ssl.ignore.hostname.verification=true This can be useful if, for example, you are using a self-signed certificate during the development phase.\nAuthentication HTTP Basic Auth You can configure HTTP Basic Auth to secure access to the Azkarra REST APIs (and Web UI). Usually, this is the simplest mechanism to implement.\n1 ) Configure your Azkarra application with the following properties :\nazkarra.server.rest.authentication.mechanism=\u0026quot;BASIC_AUTH\u0026quot; azkarra.server.rest.authentication.realm=\u0026quot;AzkarraServer\u0026quot; azkarra.server.rest.authentication.roles=\u0026quot;admin, alice\u0026quot; 2 ) Then add the users. Azkarra supports two approaches for doing that :\nJAAS Configuration Azkarra supports the Java Authentication and Authorization Service (JAAS) and provides a PropertiesFileLoginModule for loading users from a properties file.\n3 ) Create a JAAS configuration file. For example /etc/azkarra/azkarra-server-jass.conf:\nAzkarraServer { io.streamthoughts.azkarra.http.security.jaas.spi.PropertiesFileLoginModule required file=\u0026quot;/etc/azkarra/azkarra.password\u0026quot; reloadInterval=\u0026quot;60\u0026quot; reload=\u0026quot;true\u0026quot; debug=\u0026quot;true\u0026quot;; }; N.B: This is important that azkarra.server.rest.authentication.realm matches the section with the JAAS file.\n4 ) Create a password files. For example /etc/azkarra/azkarra.password:\nadmin:MD5:fa0deb5e70ca0c27c04b717a9c60d657,Administrator alice:alice-secret,Developer N.B: You can passe a password as a hash MD5 (e.g : echo -n “admin-secret” | md5sum).\n5 ) Pass the JAAS config file location as JVM parameter. For example:\n-Djava.security.auth.login.config=/path/to/azkarra-server-jaas.conf UserIdentityManager The approach is to configure a UserIdentityManager using the server configuration:\nazkarra.server.user.identity.manager.class Azkarra only provides a simple implementation named InMemoryUserIdentityManager. This class accepts a single configuration property rest.authentication.users that must contain the list of users separated by a comma.\nYou can implement a custom UserIdentityManager, for example, to load users from an external database.\nHere is a complete code example using a programmatic configuration style:\n@AzkarraStreamsApplication public class BasicAuthenticationExample { public static void main(final String[] args) { final Conf serverConfig = ServerConfBuilder.newBuilder() .setAuthenticationMethod( SecurityMechanism.BASIC_AUTH.name() ) .setAuthenticationRealm(\u0026#34;AzkarraServer\u0026#34;) .setAuthenticationRoles(\u0026#34;admin, alice\u0026#34;) .setUserIdentityManager( InMemoryUserIdentityManager.class ) .setAuthenticationUsers( \u0026#34;admin:admin-secret, alice:alice-secret\u0026#34; ) .build(); new AzkarraApplication() .setConfiguration(AzkarraConf.create(\u0026#34;application\u0026#34;)) .enableHttpServer(true, serverConfig) .run(args); } } N.B: Basic Authentication does not protect credentials transmitted over the network, you will typically use it in conjunction with TLS (SSL).\nSSL Client Certificate Authentication Azkarra also supports authentication using client certification (also called SSL Two-Way Authentication). This allows Azkarra to verify the identity of the client by requesting the client to issue a trusted certificate (i.e: a certificate signed by a certificate authority).\nIf you want to enable Client Certificate authentication, configure your application with the following property :\nazkarra.server.rest.authentication.mechanism=\u0026quot;CLIENT_CERT_AUTH\u0026quot; Authorization Azkarra implements a very simple and pluggable authorization mechanism via the interface AuthorizationManager.\nIn Azkarra, a user is ALLOW or DENY to perform an operation (GET, POST, PUT, etc) on a resource (e.g: /streams/:id).\nYou can configure the authorization manager using the server property :\n   Property Type Description     azkarra.server.authorization.manager.class string The fully qualified name of the class implementing AuthorizationManager to be used to authorize an authenticated user.    By default, Azkarra provides a simple authorization manager implementation so-called SimpleAuthorizationManager. This class can be configured using the following properties :\n   Property Type Description     azkarra.server.rest.authentication.roles string The list of users or roles to authorize separated by a comma (default: *).   azkarra.server.auth.restricted.roles string The list of users or roles to restrict access    N.B: You can use a wildcard(*) to authorize all authenticated users.\nA restricted user will only be allowed to perform GET HTTP requests.\nAzkarraPrincipalBuilder The principal used to authorize a user depends on the mechanism used for authentication. For example, the principal attached to a user authenticated using SSL Client is built from the X509Certificate subject (e.g: CN=localhost,OU=Unknown,O=Unknown,L=Unknown,ST=Unknown,C=Unknown).\nFor customizing the principal name, you can configure an AzkarraPrincipalBuilder using the server configuration :\n   Property Type Description     azkarra.server.principal.builder.class string The fully qualified name of the class implementing AzkarraPrincipalBuilder.    ","excerpt":"Why Azkarra needs to be secured? Security is one of the most important concerns (if not the most …","ref":"https://streamthoughts.github.io/azkarra-streams/docs/developer-guide/security/","title":"Security"},{"body":"","excerpt":"","ref":"https://streamthoughts.github.io/azkarra-streams/docs/reference/","title":"Reference"},{"body":"Contributing Any feedback, bug reports and PRs are greatly appreciated!\n Source Code: https://github.com/streamthoughts/azkarra-streams Issue Tracker: https://github.com/streamthoughts/azkarra-streams/issues Coding Guidelines: https://github.com/streamthoughts/azkarra-streams/blob/master/CONTRIBUTING.md  ","excerpt":"Contributing Any feedback, bug reports and PRs are greatly appreciated!\n Source Code: …","ref":"https://streamthoughts.github.io/azkarra-streams/docs/contribution-guidelines/","title":"Contribution Guidelines"},{"body":"This section is where the user documentation for Azkrarra Streams lives - all the information that users need to understand and successfully use Azkarra Streams.\n","excerpt":"This section is where the user documentation for Azkrarra Streams lives - all the information that …","ref":"https://streamthoughts.github.io/azkarra-streams/docs/","title":"Documentation"},{"body":"","excerpt":"","ref":"https://streamthoughts.github.io/azkarra-streams/blog/news/","title":"News About Azkarra Streams"},{"body":"","excerpt":"","ref":"https://streamthoughts.github.io/azkarra-streams/blog/releases/","title":"New Releases"},{"body":"Azkarra Streams v0.4 is now available!\nBelow is a summary of the issues addressed in the 0.4 release of Azkarra Streams.\nStreams New Feature\n [AZKARRA-5] - Add new option enable.wait.for.topics to wait for source topics to be created before starting Kafka Streams instance.  API Improvement\n [AZKARRA-3] - MapConf should support type conversion. This fixes a ClassCastException that was thrown when arguments were passed to the application (e.g \u0026ndash;azkarra.server.port 8082).  UI New Feature\n [AZKARRA-11] - Add new option server.enable.ui for disable/enable Web UI.  Improvement\n [AZKARRA-9] - Prevent browser from opening auth popup when Basic authentication is configured.  Bug\n [Javascript] - Prevent the HTML form used to query state stores from being submitted.  Security New Feature\n [AZKARRA-7] - Add new option rest.authentication.basic.silent to enable silent basic authentication (default false).  Improvement\n [AZKARRA-1] - Obfuscate sensitive configuration (i.e password) returned in JSON HTTP response.  ","excerpt":"Azkarra Streams v0.4 is now available!\nBelow is a summary of the issues addressed in the 0.4 release …","ref":"https://streamthoughts.github.io/azkarra-streams/blog/2019/12/02/release-v0.4/","title":"Release v0.4"},{"body":"Endpoints for retrieving information about running streams applications and state stores.\nBase URL: /api/v1/applications\nGET /api/v1/applications Get a list of streams instances currently active on the locally JVM.\nExample Request\nGET /api/v1/applications/ Host: localhost:8080 Example Response\n[ \u0026#34;word-count-topology-1-0\u0026#34; ] GET /api/v1/applications/(string: applicationId) Get information about the servers, including topic/partitions assignments and available states stores, for the streams application.\nExample Request\nGET /api/v1/applications/word-count-topology-1-0 Host: localhost:8080 Example Response\n[ \u0026#34;word-count-topology-1-0\u0026#34; ] GET /api/v1/applications/(string: applicationId)/topology Get the topology DAG for the streams application.\nExample Request\nGET /api/v1/applications/word-count-topology-1-0/topology Host: localhost:8080 Example Response\nPOST /api/v1/applications/(string: applicationId)/stores/(string: name) Execute an interactive queries\nRequest JSON Object:\n type (string): The type of queried state store. Supported values are : [key_value, session, window, timestamped_key_value, timestamped_window]. set_options (map[string, String] : Options used to execute the query  retries : The maximum number of attempts to run after failed access to a given local state store. retry_backoff_ms : The time to wait before attempting to retry a failed access to a given local state store. query_timeout_ms : This limit the total time of state store execute. remote_access_allowed : Is remote access is allowed for this execute.   query: (map[string, String]) : The query clause and parameters.  Currently, Azkarra supports the following queries :\n  all : Get all key-value pairs in the specified store\n supported store types: [key_value]. (no parameter)    count : Approximate the number of entries in the specified store\n supported store types: [key_value]. (no parameter)    fetch : Get the value corresponding to the specified key from the time window (in millisecond).\n supported store types : [window]. parameters :  key time      fetch_key_range : Get all the key-value pairs in the given key range and time range from all the existing windows.\n supported store types : [window]. parameters :  key_from key_to time_from time_to      get : Get the value corresponding to the specified key.\n supported store types : [key_value]. parameters :  key      range : Get all the key-value pairs in the given key range.\n supported store types : [key_value]. parameters :  key_from key_to      Example Request\nPOST /api/v1/applications/word-count-topology-1-0/stores/count Host: localhost:8080 { \u0026#34;query\u0026#34; : { \u0026#34;get\u0026#34;: { \u0026#34;key\u0026#34;: \u0026#34;streams\u0026#34; } }, \u0026#34;type\u0026#34;:\u0026#34;key_value\u0026#34;, \u0026#34;set_options\u0026#34;:{} } Example Response\n{ \u0026#34;timeout\u0026#34;: false, \u0026#34;server\u0026#34;: \u0026#34;localhost:8080\u0026#34;, \u0026#34;took\u0026#34;: 10, \u0026#34;result\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;failure\u0026#34;: [], \u0026#34;success\u0026#34;: [ { \u0026#34;remote\u0026#34;: false, \u0026#34;total\u0026#34;: 1, \u0026#34;records\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;streams\u0026#34;, \u0026#34;value\u0026#34;: 3 } ], \u0026#34;server\u0026#34;: \u0026#34;localhost:8080\u0026#34; } ] }, \u0026#34;status\u0026#34;: \u0026#34;SUCCESS\u0026#34; } ","excerpt":"Endpoints for retrieving information about running streams applications and state stores.\nBase URL: …","ref":"https://streamthoughts.github.io/azkarra-streams/docs/reference/rest-apis/application/","title":"Applications"},{"body":"The latest Azkarra Streams API (0.5.0) documentation can be found on here.\nPrior releases : 0.3, 0.4, 0.5.0\n","excerpt":"The latest Azkarra Streams API (0.5.0) documentation can be found on here.\nPrior releases : 0.3, …","ref":"https://streamthoughts.github.io/azkarra-streams/docs/reference/api-docs/","title":"Azkarra Streams API Docs"},{"body":"Azkarra Streams embeds an HTTP server which exposes a REST API to manage available topologies and streams jobs running locally. By default, the server listens on port 8080.\nContent-Type Currently, the REST API only supports application/json as both the request and response entity content type.\n","excerpt":"Azkarra Streams embeds an HTTP server which exposes a REST API to manage available topologies and …","ref":"https://streamthoughts.github.io/azkarra-streams/docs/reference/rest-apis/","title":"Azkarra Streams REST Interface"},{"body":"Endpoints for retrieving information about local environments.\nBase URL: /api/v1/environments\n ","excerpt":"Endpoints for retrieving information about local environments.\nBase URL: /api/v1/environments\n ","ref":"https://streamthoughts.github.io/azkarra-streams/docs/reference/rest-apis/environment/","title":"Environment"},{"body":"Endpoints for retrieving information about the application health.\nBase URL: /health\nGET /health Returns the information about the application.\nExample Request\nGET /health Host: localhost:8080 Example Response\n{ \u0026#34;status\u0026#34;: \u0026#34;UP\u0026#34;, \u0026#34;details\u0026#34;: { \u0026#34;applications\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;applications\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;UP\u0026#34;, \u0026#34;details\u0026#34;: { \u0026#34;basic-word-count-topology-0-3\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;UP\u0026#34;, \u0026#34;details\u0026#34;: { \u0026#34;state\u0026#34;: \u0026#34;RUNNING\u0026#34; } } } } } } ","excerpt":"Endpoints for retrieving information about the application health.\nBase URL: /health\nGET /health …","ref":"https://streamthoughts.github.io/azkarra-streams/docs/reference/rest-apis/health/","title":"Health"},{"body":"Endpoints for retrieving information about the application.\nBase URL: /info\nGET /info Returns the information about the application.\nExample Request\nGET /info Host: localhost:8080 Example Response\n{ app: { java: { version: \u0026#34;11.0.1\u0026#34; }, name: \u0026#34;azkarra-quickstart-java\u0026#34;, description: \u0026#34;Simple Azkarra Streams project\u0026#34;, encoding: \u0026#34;UTF-8\u0026#34;, version: \u0026#34;{{ site.current_version }}\u0026#34; } } ","excerpt":"Endpoints for retrieving information about the application.\nBase URL: /info\nGET /info Returns the …","ref":"https://streamthoughts.github.io/azkarra-streams/docs/reference/rest-apis/info/","title":"Info"},{"body":"Endpoints for retrieving information about streams instances running locally.\nBase URL: /api/v1/streams\nGET /api/v1/streams Get a list of streams instances currently active on the local JVM.\nExample Request\nGET /api/v1/streams Host: localhost:8080 Example Response\n[ \u0026#34;word-count-topology-1-0\u0026#34; ] GET /api/v1/streams/(string: applicationId) Get information about the local streams instance.\nExample Request\nGET /api/v1/streams/word-count-topology-1-0 Host: localhost:8080 Example Response\n{ \u0026#34;since\u0026#34;: \u0026#34;2019-10-04T14:06:06.137+02:00[Europe/Paris]\u0026#34;, \u0026#34;state\u0026#34;: { \u0026#34;since\u0026#34;: \u0026#34;2019-10-04T14:06:06.461+02:00[Europe/Paris]\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;RUNNING\u0026#34; }, \u0026#34;version\u0026#34;: \u0026#34;1.0\u0026#34;, \u0026#34;description\u0026#34;: null, \u0026#34;config\u0026#34;: { \u0026#34;bootstrap.servers\u0026#34;: \u0026#34;localhost:9092\u0026#34;, \u0026#34;application.server\u0026#34;: \u0026#34;localhost:8080\u0026#34;, \u0026#34;log.maxFileSize\u0026#34;: 104857600, \u0026#34;default.value.serde\u0026#34;: \u0026#34;org.apache.kafka.common.serialization.Serdes$StringSerde\u0026#34;, \u0026#34;rocksdb.config.setter\u0026#34;: \u0026#34;io.streamthoughts.azkarra.api.streams.rocksdb.DefaultRocksDBConfigSetter\u0026#34;, \u0026#34;state.dir\u0026#34;: \u0026#34;/tmp/kafka-streams/\u0026#34;, \u0026#34;stats.enable\u0026#34;: false, \u0026#34;default.key.serde\u0026#34;: \u0026#34;org.apache.kafka.common.serialization.Serdes$StringSerde\u0026#34;, \u0026#34;log.dir\u0026#34;: \u0026#34;/var/log/kafka-streams/rocksdb\u0026#34;, \u0026#34;stats.dumpPeriodSec\u0026#34;: 30, \u0026#34;application.id\u0026#34;: \u0026#34;word-count-topology-1-0\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;WordCountTopology\u0026#34; } GET /api/v1/streams/(string: applicationId)/status Get current status about the running tasks for the streams application.\nExample Request\nGET /api/v1/streams/word-count-topology-1-0/tasks Host: localhost:8080 Example Response\nGET /api/v1/streams/(string: applicationId)/config Get the configuration for the streams application.\nExample Request\nGET /api/v1/streams/word-count-topology-1-0/config Host: localhost:8080 Example Response\n{ \u0026#34;stats.dumpPeriodSec\u0026#34;: 30, \u0026#34;default.value.serde\u0026#34;: \u0026#34;org.apache.kafka.common.serialization.Serdes$StringSerde\u0026#34;, \u0026#34;rocksdb.config.setter\u0026#34;: \u0026#34;io.streamthoughts.azkarra.api.streams.rocksdb.DefaultRocksDBConfigSetter\u0026#34;, \u0026#34;bootstrap.servers\u0026#34;: \u0026#34;localhost:9092\u0026#34;, \u0026#34;application.server\u0026#34;: \u0026#34;localhost:8080\u0026#34;, \u0026#34;state.dir\u0026#34;: \u0026#34;/tmp/kafka-streams/\u0026#34;, \u0026#34;stats.enable\u0026#34;: false, \u0026#34;log.maxFileSize\u0026#34;: 104857600, \u0026#34;default.key.serde\u0026#34;: \u0026#34;org.apache.kafka.common.serialization.Serdes$StringSerde\u0026#34;, \u0026#34;log.dir\u0026#34;: \u0026#34;/var/log/kafka-streams/rocksdb\u0026#34;, \u0026#34;application.id\u0026#34;: \u0026#34;word-count-topology-1-0\u0026#34; } GET /api/v1/streams/(string: applicationId)/metrics Get current metrics for the streams application.\nAccepted query parameters :\n   Parameter/Value Description     format=prometheus Get streams metrics for Prometheus scrapping   filter_empty Filter all streams metrics with empty value    Example Request\nGET /api/v1/streams/word-count-topology-1-0/metrics Host: localhost:8080 Example Response\n{ [{ \u0026#34;name\u0026#34;: \u0026#34;consumer-fetch-manager-metrics\u0026#34;, \u0026#34;metrics\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;bytes-consumed-rate\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The average number of bytes consumed per second for a topic\u0026#34;, \u0026#34;tags\u0026#34;: { \u0026#34;client-id\u0026#34;: \u0026#34;word-count-topology-1-0-5f27b08e-f7a2-408e-b8d1-72ddf4d9adc6-StreamThread-1-consumer\u0026#34;, \u0026#34;topic\u0026#34;: \u0026#34;word-count-topology-1-0-count-repartition\u0026#34; }, \u0026#34;value\u0026#34;: 0 }, { \u0026#34;name\u0026#34;: \u0026#34;bytes-consumed-total\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The total number of bytes consumed\u0026#34;, \u0026#34;tags\u0026#34;: { \u0026#34;client-id\u0026#34;: \u0026#34;word-count-topology-1-0-5f27b08e-f7a2-408e-b8d1-72ddf4d9adc6-StreamThread-1-consumer\u0026#34; }, \u0026#34;value\u0026#34;: 0 }, { \u0026#34;name\u0026#34;: \u0026#34;fetch-latency-avg\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The average time taken for a fetch request.\u0026#34;, \u0026#34;tags\u0026#34;: { \u0026#34;client-id\u0026#34;: \u0026#34;word-count-topology-1-0-5f27b08e-f7a2-408e-b8d1-72ddf4d9adc6-StreamThread-1-consumer\u0026#34; }, \u0026#34;value\u0026#34;: 501.46875 }, ... }] } GET /api/v1/streams/(string: applicationId)/metrics/group/{group} Get current metrics for the streams application and metric group.\nAccepted query parameters :\n   Parameter/Value Description     format=prometheus Get streams metrics for Prometheus scrapping   filter_empty Filter all streams metrics with empty value    Example Request\nGET /api/v1/streams/word-count-topology-1-0/metrics/metrics/group/app-info Host: localhost:8080 GET /api/v1/streams/(string: applicationId)/metrics/group/{group}/name/{name} Get current metrics for the streams application, metric group and name.\nAccepted query parameters :\n   Parameter/Value Description     format=prometheus Get streams metrics for Prometheus scrapping   filter_empty Filter all streams metrics with empty value    Example Request\nGET /api/v1/streams/word-count-topology-1-0/metrics/metrics/group/app-info/name/version Host: localhost:8080 GET /api/v1/streams/(string: applicationId)/metrics/group/{group}/name/{name}/value Get current metrics for the streams application, metric group and name.\nExample Request\nGET /api/v1/streams/word-count-topology-1-0/metrics/metrics/group/app-info/name/version/value Host: localhost:8080 POST /api/v1/streams/(string: applicationId)/restart Restart the local active streams instance.\nExample Request\nPOST /api/v1/streams/word-count-topology-1-0/restart Host: localhost:8080 Example Response\nRESPONSE 200/OK DELETE /api/v1/streams/(string: applicationId)/stop Stop the local active streams instance.\nRequest JSON Object:\n cleanup (boolean): Flag to indicate if the local streams states should be cleaned up.  Example Request\nDELETE /api/v1/streams/word-count-topology-1-0/stop Host: localhost:8080 { cleanup: false } Example Response\nRESPONSE 200/OK ","excerpt":"Endpoints for retrieving information about streams instances running locally.\nBase URL: …","ref":"https://streamthoughts.github.io/azkarra-streams/docs/reference/rest-apis/streams/","title":"Streams"},{"body":"Endpoints for retrieving information about available topologies.\nBase URL: /api/v1/topologies\nGET /api/v1/topologies Returns a list of Topologies available on the local application.\nExample Request\nGET /api/v1/topologies Host: localhost:8080 Example Response\n[ { \u0026#34;name\u0026#34;: \u0026#34;io.streamthoughts.azkarra.example.topology.WordCountTopology\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Kafka Streams WordCount Demo\u0026#34;, \u0026#34;aliases\u0026#34;: [ \u0026#34;WordCount\u0026#34;, \u0026#34;WordCountTopology\u0026#34; ] } ] ","excerpt":"Endpoints for retrieving information about available topologies.\nBase URL: /api/v1/topologies\nGET …","ref":"https://streamthoughts.github.io/azkarra-streams/docs/reference/rest-apis/topology/","title":"Topologies"},{"body":"Endpoints for retrieving information about Azkarra version.\nBase URL: /version\nGET /version Returns the current version of Azkarra\nExample Request\nGET /version Host: localhost:8080 Example Response\n{ \u0026#34;azkarraVersion\u0026#34;: \u0026#34;0.1\u0026#34; } ","excerpt":"Endpoints for retrieving information about Azkarra version.\nBase URL: /version\nGET /version Returns …","ref":"https://streamthoughts.github.io/azkarra-streams/docs/reference/rest-apis/versions/","title":"Version"},{"body":"Today, we are excited to announce Azkarra Streams, a new open-source micro Java framework that let you focus on writing Kafka Streams topologies code, not boilerplate code necessary for executing them.\nPhoto: Azkarra Streams Logo\n  The complete blog post can found here : introducing-azkarra-streams-the-first-micro-framework-for-apache-kafka-streams\n","excerpt":"Today, we are excited to announce Azkarra Streams, a new open-source micro Java framework that let …","ref":"https://streamthoughts.github.io/azkarra-streams/blog/2019/11/27/introducing-azkarra-streams/","title":"Introducing Azkarra Streams"},{"body":"Azkarra Streams v0.3 is now available!\n","excerpt":"Azkarra Streams v0.3 is now available!","ref":"https://streamthoughts.github.io/azkarra-streams/blog/2019/11/21/release-v0.3/","title":"Release v0.3"},{"body":"","excerpt":"","ref":"https://streamthoughts.github.io/azkarra-streams/index.json","title":""},{"body":"  .imagesizing { width:auto; text-align:center; padding:10px; } img { max-width:100%; height:auto; }  Welcome to Azkarra Streams! Create Kafka Streams applications faster than ever before!\nGet Started   View Repository          Easy to use API. Azkarra Streams is a lightweight Java framework which makes easy to develop and operate Kafka Streams applications (Azkarra is Basque word for \"Fast\")  Highlights  No more boilerplate code for running Kafka Streams.  Easy externalization of configurations (using Typesafe Config).  Web UI for topologies visualization.  Provides REST endpoints for managing and monitoring local streams instances.  Provides REST endpoints for Interactive Queries.  Encryption and Authentication with SSL or Basic Auth.         Developer-friendly features  REST API for Interactive Queries Embedded Web UI (DAG visualization) Auto create topics     Operations  Healthcheck Metrics (Prometheus, JSON) Dead Letter Topic RocksDB Tuning     Security  Headless Mode TLS/SSL Encryption Basic Authentication Client Certification Authentication         Contributions welcome Contribute  Want to join the fun on Github? New users are always welcome!\n   Support Azkarra Team Star  Want to support the development team? Add a star to the GitHub project, it only takes 5 seconds!\n    ","excerpt":".imagesizing { width:auto; text-align:center; padding:10px; } img { max-width:100%; height:auto; } …","ref":"https://streamthoughts.github.io/azkarra-streams/","title":"Azkarra Streams"},{"body":"","excerpt":"","ref":"https://streamthoughts.github.io/azkarra-streams/community/","title":"Community"},{"body":"This is the blog section. It has two categories: News and Releases.\nFiles in these directories will be listed in reverse chronological order.\n","excerpt":"This is the blog section. It has two categories: News and Releases.\nFiles in these directories will …","ref":"https://streamthoughts.github.io/azkarra-streams/blog/","title":"Docsy Blog"},{"body":"","excerpt":"","ref":"https://streamthoughts.github.io/azkarra-streams/search/","title":"Search Results"}]